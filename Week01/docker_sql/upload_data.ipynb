{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ab2b16c1-1f22-415e-b9ca-3b99b49d57b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting psycopg2\n",
      "  Using cached psycopg2-2.9.10.tar.gz (385 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hBuilding wheels for collected packages: psycopg2\n",
      "  Building wheel for psycopg2 (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for psycopg2: filename=psycopg2-2.9.10-cp312-cp312-linux_x86_64.whl size=168407 sha256=9d65e92462440c24b8c871987f8915bab19e1a65af54dd98ef048eda466f5791\n",
      "  Stored in directory: /home/horeb/.cache/pip/wheels/ac/bb/ce/afa589c50b6004d3a06fc691e71bd09c9bd5f01e5921e5329b\n",
      "Successfully built psycopg2\n",
      "Installing collected packages: psycopg2\n",
      "Successfully installed psycopg2-2.9.10\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install psycopg2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3ff4cf92-574b-4b3f-bba9-247d365d1cdd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.2.2'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Libraries used \n",
    "import pandas as pd\n",
    "import requests \n",
    "from io import BytesIO \n",
    "from sqlalchemy import create_engine\n",
    "from time import time\n",
    "\n",
    "#check pandas version \n",
    "pd.__version__\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7109f371-bf0a-453d-8560-9bfcfc763f97",
   "metadata": {},
   "source": [
    "## Import the parquet file from the site and convert it on csv to save "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1ab42d48-b722-4442-989e-e40bd228f769",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File saved on : yellow_tripdata_2024-01.csv\n"
     ]
    }
   ],
   "source": [
    "# TO RUN ONCE !!!\n",
    "\n",
    "# The database we need to import is on parquet form at https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2023-01.parquet\n",
    "url = \"https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2023-01.parquet\"\n",
    "\n",
    "# Dowload the file and get the csv \n",
    "response = requests.get(url)\n",
    "print(response.content)\n",
    "if response.status_code == 200: \n",
    "    parquet_file = BytesIO(response.content)\n",
    "    df = pd.read_parquet(parquet_file, engine =\"pyarrow\")\n",
    "\n",
    "    # Get the CSV \n",
    "    csv_file = \"yellow_tripdata_2024-01.csv\"\n",
    "    df.to_csv(csv_file, index=False)\n",
    "    print(f\"File saved on : {csv_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0ef6e08-4766-4c48-91ca-7da7cc955aec",
   "metadata": {},
   "source": [
    "## Import the CSV file, get the schema and ingestion in PostgreSQL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0af96325-b352-4932-b387-c53afa30862f",
   "metadata": {},
   "source": [
    "### Try with the 100 first lines of the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9d31a28c-5244-4271-aeb3-42a97249afdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CREATE TABLE \"yellow_taxi_data\" (\n",
      "\"VendorID\" INTEGER,\n",
      "  \"tpep_pickup_datetime\" TIMESTAMP,\n",
      "  \"tpep_dropoff_datetime\" TIMESTAMP,\n",
      "  \"passenger_count\" REAL,\n",
      "  \"trip_distance\" REAL,\n",
      "  \"RatecodeID\" REAL,\n",
      "  \"store_and_fwd_flag\" TEXT,\n",
      "  \"PULocationID\" INTEGER,\n",
      "  \"DOLocationID\" INTEGER,\n",
      "  \"payment_type\" INTEGER,\n",
      "  \"fare_amount\" REAL,\n",
      "  \"extra\" REAL,\n",
      "  \"mta_tax\" REAL,\n",
      "  \"tip_amount\" REAL,\n",
      "  \"tolls_amount\" REAL,\n",
      "  \"improvement_surcharge\" REAL,\n",
      "  \"total_amount\" REAL,\n",
      "  \"congestion_surcharge\" REAL,\n",
      "  \"airport_fee\" REAL\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Import the first 100 lines of the database \n",
    "df = pd.read_csv('yellow_tripdata_2024-01.csv', nrows=100)\n",
    "\n",
    "# When we display the schema, we note that date variables are not at the right format. \n",
    "# Let's right format them \n",
    "df.tpep_pickup_datetime = pd.to_datetime(df.tpep_pickup_datetime)\n",
    "df.tpep_dropoff_datetime = pd.to_datetime(df.tpep_dropoff_datetime)\n",
    "\n",
    "# Get the schema \n",
    "print(pd.io.sql.get_schema(df, name = 'yellow_taxi_data'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f902454a-8f35-4ec3-992a-3437ef4f53ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nCREATE TABLE yellow_taxi_data (\\n\\t\"VendorID\" BIGINT, \\n\\ttpep_pickup_datetime TIMESTAMP WITHOUT TIME ZONE, \\n\\ttpep_dropoff_datetime TIMESTAMP WITHOUT TIME ZONE, \\n\\tpassenger_count FLOAT(53), \\n\\ttrip_distance FLOAT(53), \\n\\t\"RatecodeID\" FLOAT(53), \\n\\tstore_and_fwd_flag TEXT, \\n\\t\"PULocationID\" BIGINT, \\n\\t\"DOLocationID\" BIGINT, \\n\\tpayment_type BIGINT, \\n\\tfare_amount FLOAT(53), \\n\\textra FLOAT(53), \\n\\tmta_tax FLOAT(53), \\n\\ttip_amount FLOAT(53), \\n\\ttolls_amount FLOAT(53), \\n\\timprovement_surcharge FLOAT(53), \\n\\ttotal_amount FLOAT(53), \\n\\tcongestion_surcharge FLOAT(53), \\n\\tairport_fee FLOAT(53)\\n)\\n\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ingestion into the database\n",
    "# Create engine to connect to PostgreSQL\n",
    "engine = create_engine('postgresql://root:root@localhost:5432/ny_taxi')\n",
    "engine.connect()\n",
    "\n",
    "# Upload DataFrame to PostgreSQL\n",
    "pd.io.sql.get_schema(df, name=\"yellow_taxi_data\", con=engine)\n",
    "## This will create the schema that helps us to correctly upload the df in the PostrgreSQL database"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0d0c6af-fd21-40ee-9d8e-3d333178ae1f",
   "metadata": {},
   "source": [
    "### Try with chunks and iteratively "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3aff76da-24af-446d-825d-e28a4c1f787d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 7.52 s, sys: 269 ms, total: 7.78 s\n",
      "Wall time: 12.8 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Create the iterative df\n",
    "df_iter = pd.read_csv('yellow_tripdata_2024-01.csv', iterator=True, chunksize=100000)\n",
    "\n",
    "# Get the current df and make formatting\n",
    "df = next(df_iter)\n",
    "df.tpep_pickup_datetime = pd.to_datetime(df.tpep_pickup_datetime)\n",
    "df.tpep_dropoff_datetime = pd.to_datetime(df.tpep_dropoff_datetime)\n",
    "\n",
    "# Insert in database. The use of %time is just to time the execution and get some interesting information\n",
    "%time df.to_sql(name=\"yellow_taxi_data\", con=engine, if_exists=\"replace\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7bd919b-a77d-4430-94a2-6a651ceade03",
   "metadata": {},
   "source": [
    "### Injesting of the whole dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "61e565b6-f14a-4e11-a705-628aa90dc1ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inserted another chunk... took 12.767 second(s)\n",
      "Inserted another chunk... took 12.742 second(s)\n",
      "Inserted another chunk... took 12.709 second(s)\n",
      "Inserted another chunk... took 12.833 second(s)\n",
      "Inserted another chunk... took 12.704 second(s)\n",
      "Inserted another chunk... took 12.590 second(s)\n",
      "Inserted another chunk... took 12.565 second(s)\n",
      "Inserted another chunk... took 12.686 second(s)\n",
      "Inserted another chunk... took 12.601 second(s)\n",
      "Inserted another chunk... took 12.710 second(s)\n",
      "Inserted another chunk... took 12.577 second(s)\n",
      "Inserted another chunk... took 12.626 second(s)\n",
      "Inserted another chunk... took 12.608 second(s)\n",
      "Inserted another chunk... took 12.609 second(s)\n",
      "Inserted another chunk... took 12.644 second(s)\n",
      "Inserted another chunk... took 12.663 second(s)\n",
      "Inserted another chunk... took 12.767 second(s)\n",
      "Inserted another chunk... took 12.667 second(s)\n",
      "Inserted another chunk... took 12.686 second(s)\n",
      "Inserted another chunk... took 12.571 second(s)\n",
      "Inserted another chunk... took 12.696 second(s)\n",
      "Inserted another chunk... took 12.633 second(s)\n",
      "Inserted another chunk... took 12.949 second(s)\n",
      "Inserted another chunk... took 12.648 second(s)\n",
      "Inserted another chunk... took 12.735 second(s)\n",
      "Inserted another chunk... took 12.695 second(s)\n",
      "Inserted another chunk... took 12.875 second(s)\n",
      "Inserted another chunk... took 12.666 second(s)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_29032/295939750.py:8: DtypeWarning: Columns (6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = next(df_iter)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inserted another chunk... took 12.713 second(s)\n",
      "Inserted another chunk... took 7.502 second(s)\n",
      "End of data importation.\n"
     ]
    }
   ],
   "source": [
    "# Ingest the remain \n",
    "while True:\n",
    "    try:\n",
    "        # Démarrer le chronomètre\n",
    "        t_start = time()\n",
    "\n",
    "        # Lire la tranche suivante\n",
    "        df = next(df_iter)\n",
    "\n",
    "        # Corriger les types de colonnes\n",
    "        df.tpep_pickup_datetime = pd.to_datetime(df.tpep_pickup_datetime)\n",
    "        df.tpep_dropoff_datetime = pd.to_datetime(df.tpep_dropoff_datetime)\n",
    "\n",
    "        # Ajouter les données dans la table existante\n",
    "        df.to_sql(name=\"yellow_taxi_data\", con=engine, if_exists=\"append\")\n",
    "\n",
    "        # Fin du chronomètre\n",
    "        t_end = time()\n",
    "\n",
    "        print('Inserted another chunk... took %.3f second(s)' % (t_end - t_start))\n",
    "\n",
    "    except StopIteration:\n",
    "        print(\"End of data importation.\")\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "21840065",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "265"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import and ingest the Taxi Zone Lookup file\n",
    "df_zone = pd.read_csv('taxi_zone_lookup.csv')\n",
    "engine = create_engine('postgresql://root:root@localhost:5432/ny_taxi')\n",
    "\n",
    "# Ingestion\n",
    "df_zone.to_sql(name='zones', con=engine, if_exists='replace')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d9fce0cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   LocationID        Borough                     Zone service_zone\n",
      "0           1            EWR           Newark Airport          EWR\n",
      "1           2         Queens              Jamaica Bay    Boro Zone\n",
      "2           3          Bronx  Allerton/Pelham Gardens    Boro Zone\n",
      "3           4      Manhattan            Alphabet City  Yellow Zone\n",
      "4           5  Staten Island            Arden Heights    Boro Zone\n"
     ]
    }
   ],
   "source": [
    "print(df_zone.head())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
