{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3ff4cf92-574b-4b3f-bba9-247d365d1cdd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.2.3'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Libraries used \n",
    "import pandas as pd\n",
    "import requests \n",
    "from io import BytesIO \n",
    "from sqlalchemy import create_engine\n",
    "from time import time\n",
    "\n",
    "#check pandas version \n",
    "pd.__version__\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7109f371-bf0a-453d-8560-9bfcfc763f97",
   "metadata": {},
   "source": [
    "## Import the parquet file from the site and convert it on csv to save "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1ab42d48-b722-4442-989e-e40bd228f769",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File saved on : yellow_tripdata_2024-01.csv\n"
     ]
    }
   ],
   "source": [
    "# TO RUN ONCE !!!\n",
    "\n",
    "# The database we need to import is on parquet form at https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2023-01.parquet\n",
    "url = \"https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2023-01.parquet\"\n",
    "\n",
    "# Dowload the file and get the csv \n",
    "response = requests.get(url)\n",
    "print(response.content)\n",
    "if response.status_code == 200: \n",
    "    parquet_file = BytesIO(response.content)\n",
    "    df = pd.read_parquet(parquet_file, engine =\"pyarrow\")\n",
    "\n",
    "    # Get the CSV \n",
    "    csv_file = \"yellow_tripdata_2024-01.csv\"\n",
    "    df.to_csv(csv_file, index=False)\n",
    "    print(f\"File saved on : {csv_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0ef6e08-4766-4c48-91ca-7da7cc955aec",
   "metadata": {},
   "source": [
    "## Import the CSV file, get the schema and ingestion in PostgreSQL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0af96325-b352-4932-b387-c53afa30862f",
   "metadata": {},
   "source": [
    "### Try with the 100 first lines of the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9d31a28c-5244-4271-aeb3-42a97249afdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CREATE TABLE \"yellow_taxi_data\" (\n",
      "\"VendorID\" INTEGER,\n",
      "  \"tpep_pickup_datetime\" TIMESTAMP,\n",
      "  \"tpep_dropoff_datetime\" TIMESTAMP,\n",
      "  \"passenger_count\" REAL,\n",
      "  \"trip_distance\" REAL,\n",
      "  \"RatecodeID\" REAL,\n",
      "  \"store_and_fwd_flag\" TEXT,\n",
      "  \"PULocationID\" INTEGER,\n",
      "  \"DOLocationID\" INTEGER,\n",
      "  \"payment_type\" INTEGER,\n",
      "  \"fare_amount\" REAL,\n",
      "  \"extra\" REAL,\n",
      "  \"mta_tax\" REAL,\n",
      "  \"tip_amount\" REAL,\n",
      "  \"tolls_amount\" REAL,\n",
      "  \"improvement_surcharge\" REAL,\n",
      "  \"total_amount\" REAL,\n",
      "  \"congestion_surcharge\" REAL,\n",
      "  \"airport_fee\" REAL\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Import the first 100 lines of the database \n",
    "df = pd.read_csv('yellow_tripdata_2024-01.csv', nrows=100)\n",
    "\n",
    "# When we display the schema, we note that date variables are not at the right format. \n",
    "# Let's right format them \n",
    "df.tpep_pickup_datetime = pd.to_datetime(df.tpep_pickup_datetime)\n",
    "df.tpep_dropoff_datetime = pd.to_datetime(df.tpep_dropoff_datetime)\n",
    "\n",
    "# Get the schema \n",
    "print(pd.io.sql.get_schema(df, name = 'yellow_taxi_data'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f902454a-8f35-4ec3-992a-3437ef4f53ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nCREATE TABLE yellow_taxi_data (\\n\\t\"VendorID\" BIGINT, \\n\\ttpep_pickup_datetime TIMESTAMP WITHOUT TIME ZONE, \\n\\ttpep_dropoff_datetime TIMESTAMP WITHOUT TIME ZONE, \\n\\tpassenger_count FLOAT(53), \\n\\ttrip_distance FLOAT(53), \\n\\t\"RatecodeID\" FLOAT(53), \\n\\tstore_and_fwd_flag TEXT, \\n\\t\"PULocationID\" BIGINT, \\n\\t\"DOLocationID\" BIGINT, \\n\\tpayment_type BIGINT, \\n\\tfare_amount FLOAT(53), \\n\\textra FLOAT(53), \\n\\tmta_tax FLOAT(53), \\n\\ttip_amount FLOAT(53), \\n\\ttolls_amount FLOAT(53), \\n\\timprovement_surcharge FLOAT(53), \\n\\ttotal_amount FLOAT(53), \\n\\tcongestion_surcharge FLOAT(53), \\n\\tairport_fee FLOAT(53)\\n)\\n\\n'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ingestion into the database\n",
    "# Create engine to connect to PostgreSQL\n",
    "engine = create_engine('postgresql://root:root@localhost:5432/ny_taxi')\n",
    "engine.connect()\n",
    "\n",
    "# Upload DataFrame to PostgreSQL\n",
    "pd.io.sql.get_schema(df, name=\"yellow_taxi_data\", con=engine)\n",
    "## This will create the schema that helps us to correctly upload the df in the PostrgreSQL database"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0d0c6af-fd21-40ee-9d8e-3d333178ae1f",
   "metadata": {},
   "source": [
    "### Try with chunks and iteratively "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3aff76da-24af-446d-825d-e28a4c1f787d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.27 s, sys: 11 ms, total: 4.28 s\n",
      "Wall time: 6.55 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Create the iterative df\n",
    "df_iter = pd.read_csv('yellow_tripdata_2024-01.csv', iterator=True, chunksize=100000)\n",
    "\n",
    "# Get the current df and make formatting\n",
    "df = next(df_iter)\n",
    "df.tpep_pickup_datetime = pd.to_datetime(df.tpep_pickup_datetime)\n",
    "df.tpep_dropoff_datetime = pd.to_datetime(df.tpep_dropoff_datetime)\n",
    "\n",
    "# Insert in database. The use of %time is just to time the execution and get some interesting information\n",
    "%time df.to_sql(name=\"yellow_taxi_data\", con=engine, if_exists=\"replace\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7bd919b-a77d-4430-94a2-6a651ceade03",
   "metadata": {},
   "source": [
    "### Injesting of the whole dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "61e565b6-f14a-4e11-a705-628aa90dc1ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inserted another chunk... took 6.651 second(s)\n",
      "Inserted another chunk... took 6.689 second(s)\n",
      "Inserted another chunk... took 6.609 second(s)\n",
      "Inserted another chunk... took 6.551 second(s)\n",
      "Inserted another chunk... took 6.750 second(s)\n",
      "Inserted another chunk... took 7.051 second(s)\n",
      "Inserted another chunk... took 7.417 second(s)\n",
      "Inserted another chunk... took 7.231 second(s)\n",
      "Inserted another chunk... took 7.479 second(s)\n",
      "Inserted another chunk... took 7.295 second(s)\n",
      "Inserted another chunk... took 6.873 second(s)\n",
      "Inserted another chunk... took 7.389 second(s)\n",
      "Inserted another chunk... took 7.232 second(s)\n",
      "Inserted another chunk... took 7.304 second(s)\n",
      "Inserted another chunk... took 7.115 second(s)\n",
      "Inserted another chunk... took 7.421 second(s)\n",
      "Inserted another chunk... took 7.248 second(s)\n",
      "Inserted another chunk... took 6.930 second(s)\n",
      "Inserted another chunk... took 7.317 second(s)\n",
      "Inserted another chunk... took 7.232 second(s)\n",
      "Inserted another chunk... took 7.443 second(s)\n",
      "Inserted another chunk... took 7.498 second(s)\n",
      "Inserted another chunk... took 7.383 second(s)\n",
      "Inserted another chunk... took 7.508 second(s)\n",
      "Inserted another chunk... took 7.009 second(s)\n",
      "Inserted another chunk... took 6.965 second(s)\n",
      "Inserted another chunk... took 7.043 second(s)\n",
      "Inserted another chunk... took 7.210 second(s)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_37250/295939750.py:8: DtypeWarning: Columns (6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = next(df_iter)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inserted another chunk... took 8.255 second(s)\n",
      "Inserted another chunk... took 4.463 second(s)\n",
      "End of data importation.\n"
     ]
    }
   ],
   "source": [
    "# Ingest the remain \n",
    "while True:\n",
    "    try:\n",
    "        # Démarrer le chronomètre\n",
    "        t_start = time()\n",
    "\n",
    "        # Lire la tranche suivante\n",
    "        df = next(df_iter)\n",
    "\n",
    "        # Corriger les types de colonnes\n",
    "        df.tpep_pickup_datetime = pd.to_datetime(df.tpep_pickup_datetime)\n",
    "        df.tpep_dropoff_datetime = pd.to_datetime(df.tpep_dropoff_datetime)\n",
    "\n",
    "        # Ajouter les données dans la table existante\n",
    "        df.to_sql(name=\"yellow_taxi_data\", con=engine, if_exists=\"append\")\n",
    "\n",
    "        # Fin du chronomètre\n",
    "        t_end = time()\n",
    "\n",
    "        print('Inserted another chunk... took %.3f second(s)' % (t_end - t_start))\n",
    "\n",
    "    except StopIteration:\n",
    "        print(\"End of data importation.\")\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "21840065",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "265"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import and ingest the Taxi Zone Lookup file\n",
    "df_zone = pd.read_csv('taxi_zone_lookup.csv')\n",
    "engine = create_engine('postgresql://root:root@localhost:5432/ny_taxi')\n",
    "\n",
    "# Ingestion\n",
    "df_zone.to_sql(name='zones', con=engine, if_exists='replace')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d9fce0cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   LocationID        Borough                     Zone service_zone\n",
      "0           1            EWR           Newark Airport          EWR\n",
      "1           2         Queens              Jamaica Bay    Boro Zone\n",
      "2           3          Bronx  Allerton/Pelham Gardens    Boro Zone\n",
      "3           4      Manhattan            Alphabet City  Yellow Zone\n",
      "4           5  Staten Island            Arden Heights    Boro Zone\n"
     ]
    }
   ],
   "source": [
    "print(df_zone.head())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DataEngVenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
